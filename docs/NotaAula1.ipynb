{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1. Objetivo Final do Projeto\n",
        "\n",
        "O objetivo é criar um modelo de Inteligência Artificial especializado em **resumir notícias**. Para isso, vamos usar uma técnica chamada **Fine-Tuning**.\n",
        "\n",
        "#### 2. O que é Fine-Tuning?\n",
        "\n",
        "Fine-tuning (ou ajuste fino) é o processo de pegar um modelo de linguagem pré-treinado (como o GPT da OpenAI) e continuar seu treinamento com um conjunto de dados específico para uma tarefa particular.\n",
        "\n",
        "*   **Modelo Base:** É um modelo gigante, como o GPT-3.5, que já foi treinado com uma quantidade massiva de texto da internet e \"sabe\" sobre linguagem, fatos, raciocínio, etc. Pense nele como um recém-formado com conhecimento geral.\n",
        "*   **Ajuste Fino:** Nós o especializamos em uma tarefa (como resumir notícias) mostrando a ele muitos exemplos de como queremos que ele execute essa tarefa. É como dar uma pós-graduação ao recém-formado.\n",
        "\n",
        "O resultado é um modelo menor, mais rápido e mais consistente para a sua tarefa específica do que o modelo base genérico.\n",
        "\n",
        "#### 3. O Pipeline de Preparação de Dados (O Foco da Aula)\n",
        "\n",
        "Para fazer o fine-tuning, precisamos de um conjunto de dados de alta qualidade no formato `(entrada, saída esperada)`. No nosso caso, seria `(texto da notícia, resumo da notícia)`. A aula demonstra como criar esse dataset do zero.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "3PbKqwlw18EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Etapa 1: Coleta de Dados Brutos (Web Scraping)**\n",
        "\n",
        "Como não temos um dataset pronto, vamos buscá-lo na internet. O processo de extrair informações de sites de forma automatizada é chamado de **Web Scraping**.\n",
        "\n",
        "*   **Ferramentas Utilizadas:**\n",
        "    *   `requests`: Uma biblioteca Python para fazer requisições HTTP, ou seja, para \"baixar\" o código HTML de uma página da web.\n",
        "    *   `BeautifulSoup`: Uma biblioteca para \"navegar\" e extrair informações de um documento HTML. Ela entende a estrutura de tags (como `<div>`, `<p>`, `<a>`) e nos permite encontrar o que queremos.\n",
        "\n",
        "*   **Processo em Duas Fases:**\n",
        "    1.  **`news-scrapper.ipynb` (Coletor de Links):** O primeiro script visita uma página principal de um portal de notícias (no caso, a seção \"World\" da CNN). Ele usa `BeautifulSoup` para encontrar todas as tags `<a>` (que representam links) e filtra para pegar apenas os links que levam a artigos de notícias. Esses links (URLs) são salvos em um arquivo de texto (`CNN_Links.txt`).\n",
        "    2.  **`get-news-content.ipynb` (Extrator de Conteúdo):** O segundo script lê o arquivo `CNN_Links.txt`. Para cada link na lista, ele visita a página do artigo, usa `BeautifulSoup` novamente para encontrar o elemento HTML que contém o texto principal da notícia (geralmente um `<div>` com uma classe específica como `article__content`) e extrai todo o texto. O resultado é salvo em um arquivo JSON (`news_contents.json`) contendo uma lista de todos os textos das notícias coletadas.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nsGtTHt42Nu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CÉLULA 1: INSTALAÇÃO E CONFIGURAÇÃO INICIAL\n",
        "# ==============================================================================\n",
        "# Esta célula combina as configurações iniciais de todos os notebooks.\n",
        "\n",
        "# Instala as bibliotecas necessárias que estavam faltando nos notebooks originais.\n",
        "!pip install requests beautifulsoup4 openai\n",
        "\n",
        "# Importa as bibliotecas que serão usadas ao longo do projeto.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # Import userdata to access Colab secrets\n",
        "\n",
        "# Monta o Google Drive para salvar nossos arquivos de forma persistente.\n",
        "# Isso corrige o problema de salvar arquivos em um ambiente temporário.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- IMPORTANTE: SUBSTITUA PELA SUA CHAVE DE API DA OPENAI ---\n",
        "# Acesse a chave de API do OpenAI armazenada no Colab Secrets Manager\n",
        "# Certifique-se de ter adicionado sua chave lá com o nome 'OPENAI_API_KEY'\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if OPENAI_API_KEY is None:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in Colab Secrets Manager.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao obter a chave de API: {e}\")\n",
        "    print(\"Por favor, adicione sua chave de API da OpenAI ao Colab Secrets Manager com o nome 'OPENAI_API_KEY'.\")\n",
        "    #exit() # You might want to exit if the key is not found\n",
        "\n",
        "client = None\n",
        "if OPENAI_API_KEY:\n",
        "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "  print(\"OpenAI client initialized successfully.\")\n",
        "else:\n",
        "  print(\"OpenAI client not initialized due to missing API key.\")\n",
        "\n",
        "\n",
        "# Define um caminho base no Google Drive para organizar todos os arquivos gerados.\n",
        "# Crie esta estrutura de pastas no seu Drive: MyDrive/FIAP/Fine-tuning/Aula 1\n",
        "base_path = \"/content/drive/MyDrive/FIAP/Fine-tuning/Aula1/\"\n",
        "os.makedirs(base_path, exist_ok=True) # Garante que o diretório exista\n",
        "\n",
        "print(f\"Configuração concluída. Os arquivos serão salvos em: {base_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0C5aaj72lHW",
        "outputId": "d8bfdcaa-e16b-4373-851d-1b082b73492b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "OpenAI client initialized successfully.\n",
            "Configuração concluída. Os arquivos serão salvos em: /content/drive/MyDrive/FIAP/Fine-tuning/Aula1/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Etapa 2: Geração dos Resumos (Outputs)**\n",
        "\n",
        "Agora temos as \"entradas\" (os textos das notícias), mas ainda precisamos das \"saídas esperadas\" (os resumos). Escrever resumos para centenas de notícias manualmente seria inviável.\n",
        "\n",
        "*   **Solução (Usando uma IA para criar dados para outra IA):**\n",
        "    *   **`generate-output-for-news.ipynb`:** Este script utiliza a **API da OpenAI**. Ele pega cada notícia do arquivo `news_contents.json`, envia para um modelo poderoso (como o `gpt-3.5-turbo`) com uma instrução clara (\"*Summarize this news article...*\") e recebe de volta um resumo gerado pela IA.\n",
        "    *   O script então junta o texto original da notícia (`story`) com o resumo gerado (`summary`) em um novo arquivo JSON (`news_summaries.json`).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKcMNCz52SXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CÉLULA 2: ETAPA 1 - WEB SCRAPING DOS LINKS (Análogo a news-scrapper.ipynb)\n",
        "# --- VERSÃO FINAL COM FILTROS AVANÇADOS DE VÍDEO E DATA ---\n",
        "# ==============================================================================\n",
        "# Esta versão exclui links de vídeo e coleta apenas notícias do dia anterior.\n",
        "\n",
        "# Adiciona as importações necessárias para manipulação de datas\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def scrape_cnn_links(url):\n",
        "    \"\"\"\n",
        "    Função que faz o scraping da CNN e retorna uma lista de links de notícias,\n",
        "    aplicando dois filtros:\n",
        "    1. Exclui qualquer URL que contenha '/video/'.\n",
        "    2. Inclui apenas URLs cuja data seja do dia anterior ao da execução.\n",
        "    \"\"\"\n",
        "    print(f\"Iniciando scraping da URL: {url}\")\n",
        "\n",
        "    # --- NOVO: LÓGICA DE DATA ---\n",
        "    # Calcula a data de ontem\n",
        "    yesterday = datetime.now() - timedelta(days=1)\n",
        "    print(f\"==> Filtro Ativado: Buscando links apenas para a data de ontem: {yesterday.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        valid_links = []\n",
        "\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "\n",
        "            # --- NOVO: FILTRO 1 - EXCLUIR VÍDEOS ---\n",
        "            # Se o link contiver '/video/', pula para a próxima iteração do loop.\n",
        "            if \"/video/\" in href:\n",
        "                continue\n",
        "\n",
        "            # --- LÓGICA DE ESTRUTURA DE URL (JÁ EXISTENTE) ---\n",
        "            path_parts = href.strip('/').split('/')\n",
        "\n",
        "            if len(path_parts) >= 3 and path_parts[0].isdigit() and len(path_parts[0]) == 4:\n",
        "\n",
        "                # --- NOVO: FILTRO 2 - VERIFICAR A DATA ---\n",
        "                try:\n",
        "                    url_year = int(path_parts[0])\n",
        "                    url_month = int(path_parts[1])\n",
        "                    url_day = int(path_parts[2])\n",
        "\n",
        "                    # Compara a data do link com a data de ontem\n",
        "                    if (url_year == yesterday.year and\n",
        "                        url_month == yesterday.month and\n",
        "                        url_day == yesterday.day):\n",
        "\n",
        "                        # Se todos os filtros passaram, o link é válido\n",
        "                        full_link = f\"https://edition.cnn.com{href}\"\n",
        "                        valid_links.append(full_link)\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    # Ignora links que parecem ter uma data mas têm um formato inválido\n",
        "                    continue\n",
        "\n",
        "        unique_links = sorted(list(set(valid_links)))\n",
        "        print(f\"Encontrados {len(unique_links)} links de notícias únicos para a data de ontem.\")\n",
        "        return unique_links\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Ocorreu um erro de rede ou HTTP: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Um erro inesperado ocorreu: {e}\")\n",
        "        return []\n",
        "\n",
        "# URL da seção \"World\" da CNN, como no notebook\n",
        "cnn_url = \"https://edition.cnn.com/world\"\n",
        "links_list = scrape_cnn_links(cnn_url)\n",
        "\n",
        "# Salva os links no caminho definido no Google Drive\n",
        "links_file_path = os.path.join(base_path, \"CNN_Links.txt\")\n",
        "\n",
        "if links_list:\n",
        "    with open(links_file_path, 'w') as file:\n",
        "        for link in links_list:\n",
        "            file.write(link + '\\n')\n",
        "    print(f\"Links salvos com sucesso em: {links_file_path}\")\n",
        "else:\n",
        "    print(\"Nenhum link correspondente aos filtros foi encontrado. O arquivo de links não foi atualizado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRGsfH6N38II",
        "outputId": "4baa4283-e470-4021-f162-9ebde7a01624"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando scraping da URL: https://edition.cnn.com/world\n",
            "==> Filtro Ativado: Buscando links apenas para a data de ontem: 2025-11-03\n",
            "Encontrados 14 links de notícias únicos para a data de ontem.\n",
            "Links salvos com sucesso em: /content/drive/MyDrive/FIAP/Fine-tuning/Aula1/CNN_Links.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Etapa 3: Formatação Final do Dataset**\n",
        "\n",
        "Os modelos de fine-tuning exigem que os dados estejam em um formato específico. Normalmente, é um arquivo **JSON Lines (`.jsonl`)**, onde cada linha é um objeto JSON independente.\n",
        "\n",
        "*   **`prepare-data.ipynb`:** O último script faz essa formatação.\n",
        "    1.  Ele lê os dados que geramos (`news_summaries.json`) e também um **dataset complementar** (um arquivo `data.jsonl` já pronto, para aumentar o volume de dados).\n",
        "    2.  Para cada par `(story, summary)`, ele cria uma string formatada que serve como um único exemplo de treinamento. O formato é crucial e geralmente segue um padrão como:\n",
        "        ```\n",
        "        \"input\": \"### SUMMARIZE THIS NEWS.\\n\\n[Texto da notícia]\\n\\n###[Resumo da notícia]###\"\n",
        "        ```\n",
        "    3.  Essa estrutura ensina o modelo: \"Quando você vir essa instrução e esse texto, a sua resposta deve ser este resumo.\"\n",
        "    4.  Todos esses exemplos formatados são salvos em um único arquivo final (`news_dataset_chat_data.json`), que está pronto para ser usado no processo de fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "hS1bSUFh2XE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CÉLULA 3: ETAPA 2 - EXTRAÇÃO DO CONTEÚDO (Análogo a get-news-content.ipynb)\n",
        "# --- VERSÃO DEFINITIVA COM EXTRAÇÃO DE JSON-LD (MÉTODO PREFERENCIAL) ---\n",
        "# ==============================================================================\n",
        "# Este scraper prioriza a extração de dados estruturados (JSON-LD) e usa o\n",
        "# scraping de HTML visual como um fallback, tornando-o extremamente robusto.\n",
        "\n",
        "def get_news_content(links_file):\n",
        "    \"\"\"\n",
        "    Lê o arquivo de links e extrai o conteúdo de cada artigo.\n",
        "    Primeiro, tenta extrair o texto limpo do JSON-LD estruturado ('articleBody').\n",
        "    Se falhar, recorre ao scraping do HTML visual como fallback.\n",
        "    \"\"\"\n",
        "    print(\"\\nIniciando extração de conteúdo com scraper definitivo (JSON-LD + Fallback)...\")\n",
        "    try:\n",
        "        with open(links_file, 'r') as file:\n",
        "            links = file.readlines()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Erro: O arquivo de links '{links_file}' não foi encontrado. Execute a Célula 2 primeiro.\")\n",
        "        return None\n",
        "\n",
        "    news_contents = []\n",
        "\n",
        "    for i, link in enumerate(links):\n",
        "        link = link.strip()\n",
        "        if not link:\n",
        "            continue\n",
        "\n",
        "        print(f\"Processando link {i+1}/{len(links)}: {link}\")\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            response = requests.get(link, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            content = None # Variável para armazenar o conteúdo encontrado\n",
        "\n",
        "            # --- TENTATIVA 1: MÉTODO PREFERENCIAL (JSON-LD) ---\n",
        "            json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
        "            for script in json_ld_scripts:\n",
        "                try:\n",
        "                    # O conteúdo do script é uma string, então precisamos carregá-lo como JSON\n",
        "                    data = json.loads(script.string)\n",
        "                    # O JSON pode ser uma lista ou um dicionário, então lidamos com ambos os casos\n",
        "                    if isinstance(data, list):\n",
        "                        for item in data:\n",
        "                            if isinstance(item, dict) and 'articleBody' in item:\n",
        "                                content = item['articleBody']\n",
        "                                print(\"  -> Sucesso: Conteúdo extraído via JSON-LD.\")\n",
        "                                break # Encontrou, sai do loop interno\n",
        "                    elif isinstance(data, dict) and 'articleBody' in data:\n",
        "                        content = data['articleBody']\n",
        "                        print(\"  -> Sucesso: Conteúdo extraído via JSON-LD.\")\n",
        "                    if content:\n",
        "                        break # Encontrou, sai do loop de scripts\n",
        "                except (json.JSONDecodeError, TypeError):\n",
        "                    # Ignora scripts malformados ou vazios e continua para o próximo\n",
        "                    continue\n",
        "\n",
        "            # --- TENTATIVA 2: MÉTODO FALLBACK (SCRAPING DE HTML) ---\n",
        "            if not content:\n",
        "                print(\"  -> Aviso: JSON-LD não encontrado ou sem 'articleBody'. Tentando fallback de HTML...\")\n",
        "                main_container = soup.find('div', class_='article__content-container')\n",
        "                if not main_container:\n",
        "                    main_container = soup.find('div', class_='article__content')\n",
        "\n",
        "                if main_container:\n",
        "                    paragraphs_elements = main_container.find_all('div', {'data-component-name': 'paragraph'})\n",
        "                    if not paragraphs_elements:\n",
        "                        paragraphs_elements = main_container.find_all('p', class_='paragraph')\n",
        "\n",
        "                    if paragraphs_elements:\n",
        "                        paragraphs_text = [p.get_text(strip=True) for p in paragraphs_elements]\n",
        "                        content = ' '.join(paragraphs_text)\n",
        "                        print(\"  -> Sucesso: Conteúdo extraído via Fallback de HTML.\")\n",
        "\n",
        "            # Adiciona o resultado final à lista\n",
        "            if content:\n",
        "                news_contents.append(content)\n",
        "            else:\n",
        "                news_contents.append(\"Conteúdo não encontrado (ambos os métodos falharam).\")\n",
        "                print(\"  -> Falha: Nenhum método conseguiu extrair o conteúdo.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  -> Erro de rede ao acessar o link: {e}\")\n",
        "            news_contents.append(f\"Erro de rede: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Erro inesperado ao processar o link: {e}\")\n",
        "            news_contents.append(f\"Erro inesperado: {e}\")\n",
        "\n",
        "    return {\"news_content\": news_contents}\n",
        "\n",
        "# Chama a função para extrair os conteúdos\n",
        "contents_data = get_news_content(links_file_path)\n",
        "\n",
        "# Apenas continua se a extração foi bem-sucedida\n",
        "if contents_data:\n",
        "    # Salva o conteúdo em um arquivo JSON no Google Drive\n",
        "    contents_file_path = os.path.join(base_path, \"news_contents.json\")\n",
        "    with open(contents_file_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(contents_data, json_file, indent=4)\n",
        "    print(f\"\\nConteúdos das notícias salvos com sucesso em: {contents_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M_EbswO8Pqr",
        "outputId": "9613ae3c-1056-4db8-a3d5-9e3dddabd12f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando extração de conteúdo com scraper definitivo (JSON-LD + Fallback)...\n",
            "Processando link 1/14: https://edition.cnn.com/2025/11/03/africa/trump-christian-killings-nigeria-intl\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 2/14: https://edition.cnn.com/2025/11/03/americas/day-of-the-dead-mexico-traditions-intl-latam\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 3/14: https://edition.cnn.com/2025/11/03/americas/jamaica-black-river-destruction-after-hurricane-melissa-intl-latam\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 4/14: https://edition.cnn.com/2025/11/03/asia/himalaya-peak-avalanche-nepal-intl\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 5/14: https://edition.cnn.com/2025/11/03/asia/japan-takaichi-north-korea-kim-jong-un-intl-hnk\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 6/14: https://edition.cnn.com/2025/11/03/asia/north-korea-kim-yong-nam-death-intl-hnk\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 7/14: https://edition.cnn.com/2025/11/03/china/china-xi-laughing-images-apec-intl-hnk\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 8/14: https://edition.cnn.com/2025/11/03/europe/louvre-heist-petty-criminals-scli-intl\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 9/14: https://edition.cnn.com/2025/11/03/europe/rome-tower-collapse-intl-scli\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 10/14: https://edition.cnn.com/2025/11/03/sport/baseball-mlb-los-angeles-dodgers-world-series-parade\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 11/14: https://edition.cnn.com/2025/11/03/sport/why-there-are-so-many-long-field-goals-in-the-nfl-this-season-including-a-new-record-setter\n",
            "  -> Erro de rede ao acessar o link: 404 Client Error: OK for url: https://edition.cnn.com/2025/11/03/sport/why-there-are-so-many-long-field-goals-in-the-nfl-this-season-including-a-new-record-setter\n",
            "Processando link 12/14: https://edition.cnn.com/2025/11/03/style/abandoned-homes-america-bryan-sansivero-photography\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 13/14: https://edition.cnn.com/2025/11/03/style/sequin-dresses-lacma-lotw\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "Processando link 14/14: https://edition.cnn.com/2025/11/03/uk/england-train-stabbing-driver-intl\n",
            "  -> Sucesso: Conteúdo extraído via JSON-LD.\n",
            "\n",
            "Conteúdos das notícias salvos com sucesso em: /content/drive/MyDrive/FIAP/Fine-tuning/Aula1/news_contents.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CÉLULA 4: ETAPA 3 - GERAÇÃO DOS RESUMOS (Análogo a generate-output-for-news.ipynb)\n",
        "# --- VERSÃO CORRIGIDA COM PARSER DE JSON ROBUSTO ---\n",
        "# ==============================================================================\n",
        "# Esta versão lida com respostas malformadas da API da OpenAI, extraindo o JSON\n",
        "# válido mesmo que ele esteja cercado por texto adicional.\n",
        "\n",
        "def summarize_news(news_file):\n",
        "    \"\"\"\n",
        "    Carrega o conteúdo das notícias e gera resumos para cada uma usando a API da OpenAI,\n",
        "    com um parser de resposta robusto para evitar erros de JSON.\n",
        "    \"\"\"\n",
        "    print(\"\\nIniciando geração de resumos com a API da OpenAI (parser robusto)...\")\n",
        "    try:\n",
        "        with open(news_file, 'r', encoding='utf-8') as file:\n",
        "            news_data = json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Erro: O arquivo '{news_file}' não foi encontrado. Execute a Célula 3 primeiro.\")\n",
        "        return None\n",
        "\n",
        "    news_contents = news_data.get('news_content', [])\n",
        "    summaries = []\n",
        "\n",
        "    for i, content in enumerate(news_contents):\n",
        "        print(f\"Gerando resumo para a notícia {i+1}/{len(news_contents)}...\")\n",
        "\n",
        "        # Filtro para não enviar conteúdo inválido ou muito curto para a API\n",
        "        if not content or len(content) < 150 or \"Conteúdo não encontrado\" in content:\n",
        "            summary_obj = {\"story\": content, \"summary\": \"Conteúdo insuficiente para resumir.\"}\n",
        "            summaries.append(summary_obj)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "              model=\"gpt-3.5-turbo\",\n",
        "              response_format={ \"type\": \"json_object\" },\n",
        "              messages=[\n",
        "                {\"role\": \"system\", \"content\": 'You are a JSON summarization bot. Your only task is to summarize the provided news article. You MUST respond with ONLY a valid JSON object in the format {\"summary\": \"your_summary_here\"}. Do not include any other text, markdown, or explanations.'},\n",
        "                {\"role\": \"user\", \"content\": f\"{content}\"}\n",
        "              ],\n",
        "              temperature=0.5,\n",
        "              max_tokens=200\n",
        "            )\n",
        "\n",
        "            raw_response_string = response.choices[0].message.content\n",
        "            summary_text = \"Erro ao extrair o resumo do JSON.\"\n",
        "\n",
        "            # --- NOVO: LÓGICA DE PARSING ROBUSTA ---\n",
        "            try:\n",
        "                # Tentativa 1: Parse direto\n",
        "                summary_data = json.loads(raw_response_string)\n",
        "                summary_text = summary_data.get(\"summary\", summary_text)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"  -> Aviso: Resposta da OpenAI não é um JSON válido. Tentando extrair...\")\n",
        "                # Tentativa 2: Extrair o JSON do meio da string\n",
        "                try:\n",
        "                    start_index = raw_response_string.find('{')\n",
        "                    end_index = raw_response_string.rfind('}') + 1\n",
        "                    if start_index != -1 and end_index != 0:\n",
        "                        clean_json_string = raw_response_string[start_index:end_index]\n",
        "                        summary_data = json.loads(clean_json_string)\n",
        "                        summary_text = summary_data.get(\"summary\", summary_text)\n",
        "                        print(\"  -> Sucesso: JSON extraído com sucesso.\")\n",
        "                    else:\n",
        "                         print(\"  -> Falha: Não foi possível encontrar um objeto JSON na resposta.\")\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"  -> Falha: Erro ao decodificar o JSON extraído. Erro: {e}\")\n",
        "                    print(f\"  -> Resposta Bruta com Problema: {raw_response_string}\")\n",
        "\n",
        "            summary_obj = {\"story\": content, \"summary\": summary_text}\n",
        "            summaries.append(summary_obj)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Erro na API da OpenAI: {e}\")\n",
        "            summaries.append({\"story\": content, \"summary\": \"Erro na chamada da API.\"})\n",
        "\n",
        "    return {\"news_summaries\": summaries}\n",
        "\n",
        "# Chama a função para gerar os resumos\n",
        "summaries_data = summarize_news(contents_file_path)\n",
        "\n",
        "# Apenas continua se a geração foi bem-sucedida\n",
        "if summaries_data:\n",
        "    # Salva os resultados (notícia + resumo) no Google Drive\n",
        "    summaries_file_path = os.path.join(base_path, \"news_summaries.json\")\n",
        "    with open(summaries_file_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(summaries_data, json_file, indent=4)\n",
        "    print(f\"\\nResumos gerados e salvos em: {summaries_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyzrXauR9Y3P",
        "outputId": "3277f996-3d35-456b-b544-d6cd38f9feab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando geração de resumos com a API da OpenAI (parser robusto)...\n",
            "Gerando resumo para a notícia 1/14...\n",
            "Gerando resumo para a notícia 2/14...\n",
            "Gerando resumo para a notícia 3/14...\n",
            "Gerando resumo para a notícia 4/14...\n",
            "Gerando resumo para a notícia 5/14...\n",
            "Gerando resumo para a notícia 6/14...\n",
            "Gerando resumo para a notícia 7/14...\n",
            "Gerando resumo para a notícia 8/14...\n",
            "Gerando resumo para a notícia 9/14...\n",
            "Gerando resumo para a notícia 10/14...\n",
            "Gerando resumo para a notícia 11/14...\n",
            "Gerando resumo para a notícia 12/14...\n",
            "Gerando resumo para a notícia 13/14...\n",
            "Gerando resumo para a notícia 14/14...\n",
            "\n",
            "Resumos gerados e salvos em: /content/drive/MyDrive/FIAP/Fine-tuning/Aula1/news_summaries.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CÉLULA 5: ETAPA 4 - FORMATAÇÃO FINAL DO DATASET (Análogo a prepare-data.ipynb)\n",
        "# ==============================================================================\n",
        "# Fiel à lógica e ao formato exato de prompt do quarto notebook.\n",
        "\n",
        "def format_final_dataset(summaries_file):\n",
        "    \"\"\"\n",
        "    Lê o arquivo com notícias e resumos e formata no padrão final para o fine-tuning.\n",
        "    \"\"\"\n",
        "    print(\"\\nFormatando dados para o fine-tuning...\")\n",
        "    processed_data = []\n",
        "\n",
        "    with open(summaries_file, 'r', encoding='utf-8') as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    news_list = json_data.get(\"news_summaries\", [])\n",
        "\n",
        "    for item in news_list:\n",
        "        story = item.get(\"story\", \"\")\n",
        "        summary = item.get(\"summary\", \"\")\n",
        "\n",
        "        # Ignora entradas inválidas ou com erro\n",
        "        if not story or not summary or \"erro\" in summary.lower() or \"insuficiente\" in summary.lower():\n",
        "            continue\n",
        "\n",
        "        # O formato do texto é exatamente o mesmo que o professor demonstrou\n",
        "        formatted_text = f\"SUMMARIZE THIS NEWS.\\n[|News|] {story}[|eNews|]\\n\\n[|summary|]{summary}[|esummary|]\"\n",
        "        processed_data.append({\"input\": formatted_text})\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Processa o arquivo de resumos que criamos\n",
        "final_data = format_final_dataset(summaries_file_path)\n",
        "\n",
        "# Salva o dataset final no formato JSON, como no notebook do professor\n",
        "output_filename = os.path.join(base_path, \"news_dataset_chat_data.json\")\n",
        "with open(output_filename, 'w', encoding='utf-8') as file:\n",
        "    # O professor salva como um grande arquivo JSON, não JSONL. Vamos manter essa fidelidade.\n",
        "    json.dump(final_data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\nPIPELINE CONCLUÍDO!\")\n",
        "print(f\"Dataset final para fine-tuning foi gerado com sucesso.\")\n",
        "print(f\"Total de exemplos formatados: {len(final_data)}\")\n",
        "print(f\"Arquivo salvo em: {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMKapYL69g-3",
        "outputId": "66add3a1-3b8b-4125-862b-a0097bf25402"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Formatando dados para o fine-tuning...\n",
            "\n",
            "PIPELINE CONCLUÍDO!\n",
            "Dataset final para fine-tuning foi gerado com sucesso.\n",
            "Total de exemplos formatados: 13\n",
            "Arquivo salvo em: /content/drive/MyDrive/FIAP/Fine-tuning/Aula1/news_dataset_chat_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Programa Análogo Completo (Google Colab)\n",
        "\n",
        "Aqui está um único notebook do Google Colab que combina e executa todas as etapas mostradas na aula. Para executá-lo, basta copiar e colar o código em um novo notebook no [Google Colab](https://colab.research.google.com/).\n",
        "\n",
        "**Instruções:**\n",
        "1.  **Obtenha uma Chave de API da OpenAI:** Você precisará criar uma conta em [platform.openai.com](https://platform.openai.com/) e gerar uma chave de API na seção \"API keys\".\n",
        "2.  **Substitua a Chave:** No código abaixo, substitua o texto `\"your_openai_api_key\"` pela sua chave real.\n",
        "3.  **Execute as Células:** Execute cada célula de código em ordem.\n"
      ],
      "metadata": {
        "id": "rbEL9If42apJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iay3JF91068"
      },
      "outputs": [],
      "source": []
    }
  ]
}